{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kMeroBWTObaY"
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "G-z40uDhOkxg"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "train_df= pd.read_excel(\"D:\\\\ResoluteAI Assignment\\\\data\\\\train.xlsx\")\n",
    "test_df= pd.read_excel(\"D:\\\\ResoluteAI Assignment\\data\\\\test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "FyC6NeRqOkqo",
    "outputId": "c08ff57c-2286-4808-ba56-6cf557c15b47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "      <th>T11</th>\n",
       "      <th>T12</th>\n",
       "      <th>T13</th>\n",
       "      <th>T14</th>\n",
       "      <th>T15</th>\n",
       "      <th>T16</th>\n",
       "      <th>T17</th>\n",
       "      <th>T18</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-70</td>\n",
       "      <td>-61</td>\n",
       "      <td>-66</td>\n",
       "      <td>-53</td>\n",
       "      <td>-51</td>\n",
       "      <td>-63</td>\n",
       "      <td>-82</td>\n",
       "      <td>-57</td>\n",
       "      <td>-76</td>\n",
       "      <td>-78</td>\n",
       "      <td>-66</td>\n",
       "      <td>-66</td>\n",
       "      <td>-61</td>\n",
       "      <td>-59</td>\n",
       "      <td>-73</td>\n",
       "      <td>-75</td>\n",
       "      <td>-63</td>\n",
       "      <td>-77</td>\n",
       "      <td>B37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-77</td>\n",
       "      <td>-74</td>\n",
       "      <td>-71</td>\n",
       "      <td>-76</td>\n",
       "      <td>-65</td>\n",
       "      <td>-63</td>\n",
       "      <td>-66</td>\n",
       "      <td>-52</td>\n",
       "      <td>-55</td>\n",
       "      <td>-75</td>\n",
       "      <td>-72</td>\n",
       "      <td>-75</td>\n",
       "      <td>-74</td>\n",
       "      <td>-61</td>\n",
       "      <td>-64</td>\n",
       "      <td>-63</td>\n",
       "      <td>-53</td>\n",
       "      <td>-63</td>\n",
       "      <td>B61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-53</td>\n",
       "      <td>-38</td>\n",
       "      <td>-55</td>\n",
       "      <td>-66</td>\n",
       "      <td>-62</td>\n",
       "      <td>-62</td>\n",
       "      <td>-65</td>\n",
       "      <td>-70</td>\n",
       "      <td>-62</td>\n",
       "      <td>-52</td>\n",
       "      <td>-56</td>\n",
       "      <td>-53</td>\n",
       "      <td>-66</td>\n",
       "      <td>-68</td>\n",
       "      <td>-72</td>\n",
       "      <td>-60</td>\n",
       "      <td>-68</td>\n",
       "      <td>-77</td>\n",
       "      <td>A19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-72</td>\n",
       "      <td>-62</td>\n",
       "      <td>-59</td>\n",
       "      <td>-65</td>\n",
       "      <td>-65</td>\n",
       "      <td>-65</td>\n",
       "      <td>-78</td>\n",
       "      <td>-82</td>\n",
       "      <td>-83</td>\n",
       "      <td>-59</td>\n",
       "      <td>-84</td>\n",
       "      <td>-60</td>\n",
       "      <td>-64</td>\n",
       "      <td>-83</td>\n",
       "      <td>-69</td>\n",
       "      <td>-72</td>\n",
       "      <td>-95</td>\n",
       "      <td>-73</td>\n",
       "      <td>A22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-67</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "      <td>-63</td>\n",
       "      <td>-59</td>\n",
       "      <td>-53</td>\n",
       "      <td>-70</td>\n",
       "      <td>-72</td>\n",
       "      <td>-71</td>\n",
       "      <td>-60</td>\n",
       "      <td>-61</td>\n",
       "      <td>-57</td>\n",
       "      <td>-54</td>\n",
       "      <td>-76</td>\n",
       "      <td>-61</td>\n",
       "      <td>-66</td>\n",
       "      <td>-71</td>\n",
       "      <td>-80</td>\n",
       "      <td>A33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   T1  T2  T3  T4  T5  T6  T7  T8  T9  T10  T11  T12  T13  T14  T15  T16  T17  \\\n",
       "0 -70 -61 -66 -53 -51 -63 -82 -57 -76  -78  -66  -66  -61  -59  -73  -75  -63   \n",
       "1 -77 -74 -71 -76 -65 -63 -66 -52 -55  -75  -72  -75  -74  -61  -64  -63  -53   \n",
       "2 -53 -38 -55 -66 -62 -62 -65 -70 -62  -52  -56  -53  -66  -68  -72  -60  -68   \n",
       "3 -72 -62 -59 -65 -65 -65 -78 -82 -83  -59  -84  -60  -64  -83  -69  -72  -95   \n",
       "4 -67 -69 -65 -63 -59 -53 -70 -72 -71  -60  -61  -57  -54  -76  -61  -66  -71   \n",
       "\n",
       "   T18 target  \n",
       "0  -77    B37  \n",
       "1  -63    B61  \n",
       "2  -77    A19  \n",
       "3  -73    A22  \n",
       "4  -80    A33  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data View\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJ9noxwAOknj",
    "outputId": "7fb69a90-b5da-4745-a601-60817a5325b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36752, 19)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ZErpDI_OkkR",
    "outputId": "e6629268-43ec-4c60-ddb5-8074ac65cbc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36752 entries, 0 to 36751\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   T1      36752 non-null  int64 \n",
      " 1   T2      36752 non-null  int64 \n",
      " 2   T3      36752 non-null  int64 \n",
      " 3   T4      36752 non-null  int64 \n",
      " 4   T5      36752 non-null  int64 \n",
      " 5   T6      36752 non-null  int64 \n",
      " 6   T7      36752 non-null  int64 \n",
      " 7   T8      36752 non-null  int64 \n",
      " 8   T9      36752 non-null  int64 \n",
      " 9   T10     36752 non-null  int64 \n",
      " 10  T11     36752 non-null  int64 \n",
      " 11  T12     36752 non-null  int64 \n",
      " 12  T13     36752 non-null  int64 \n",
      " 13  T14     36752 non-null  int64 \n",
      " 14  T15     36752 non-null  int64 \n",
      " 15  T16     36752 non-null  int64 \n",
      " 16  T17     36752 non-null  int64 \n",
      " 17  T18     36752 non-null  int64 \n",
      " 18  target  36752 non-null  object\n",
      "dtypes: int64(18), object(1)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Data Info\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "ToGONOHPOkZa",
    "outputId": "40c1e75e-f5e9-49a6-aaab-ee56a04fb139"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "      <th>T11</th>\n",
       "      <th>T12</th>\n",
       "      <th>T13</th>\n",
       "      <th>T14</th>\n",
       "      <th>T15</th>\n",
       "      <th>T16</th>\n",
       "      <th>T17</th>\n",
       "      <th>T18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "      <td>36752.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-65.865449</td>\n",
       "      <td>-64.521931</td>\n",
       "      <td>-64.574472</td>\n",
       "      <td>-65.296474</td>\n",
       "      <td>-64.462152</td>\n",
       "      <td>-63.318731</td>\n",
       "      <td>-67.030202</td>\n",
       "      <td>-66.591260</td>\n",
       "      <td>-65.692479</td>\n",
       "      <td>-65.555181</td>\n",
       "      <td>-65.681922</td>\n",
       "      <td>-66.244204</td>\n",
       "      <td>-63.962614</td>\n",
       "      <td>-64.399080</td>\n",
       "      <td>-64.550011</td>\n",
       "      <td>-64.136782</td>\n",
       "      <td>-65.910726</td>\n",
       "      <td>-66.590417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.737286</td>\n",
       "      <td>8.914559</td>\n",
       "      <td>8.154517</td>\n",
       "      <td>8.142803</td>\n",
       "      <td>8.068375</td>\n",
       "      <td>8.651501</td>\n",
       "      <td>9.257529</td>\n",
       "      <td>10.160193</td>\n",
       "      <td>10.598247</td>\n",
       "      <td>10.408147</td>\n",
       "      <td>9.453958</td>\n",
       "      <td>8.866239</td>\n",
       "      <td>8.254217</td>\n",
       "      <td>9.562839</td>\n",
       "      <td>9.207343</td>\n",
       "      <td>10.540542</td>\n",
       "      <td>10.071973</td>\n",
       "      <td>10.600992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "      <td>-95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-72.000000</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>-69.000000</td>\n",
       "      <td>-69.000000</td>\n",
       "      <td>-74.000000</td>\n",
       "      <td>-75.000000</td>\n",
       "      <td>-73.000000</td>\n",
       "      <td>-73.000000</td>\n",
       "      <td>-71.000000</td>\n",
       "      <td>-72.000000</td>\n",
       "      <td>-69.000000</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>-70.000000</td>\n",
       "      <td>-71.000000</td>\n",
       "      <td>-72.000000</td>\n",
       "      <td>-74.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-66.000000</td>\n",
       "      <td>-65.000000</td>\n",
       "      <td>-64.000000</td>\n",
       "      <td>-66.000000</td>\n",
       "      <td>-65.000000</td>\n",
       "      <td>-63.000000</td>\n",
       "      <td>-66.000000</td>\n",
       "      <td>-66.000000</td>\n",
       "      <td>-65.000000</td>\n",
       "      <td>-65.000000</td>\n",
       "      <td>-65.000000</td>\n",
       "      <td>-66.000000</td>\n",
       "      <td>-63.000000</td>\n",
       "      <td>-64.000000</td>\n",
       "      <td>-65.000000</td>\n",
       "      <td>-63.000000</td>\n",
       "      <td>-65.000000</td>\n",
       "      <td>-66.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-61.000000</td>\n",
       "      <td>-59.000000</td>\n",
       "      <td>-59.000000</td>\n",
       "      <td>-61.000000</td>\n",
       "      <td>-59.000000</td>\n",
       "      <td>-57.000000</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>-59.000000</td>\n",
       "      <td>-58.000000</td>\n",
       "      <td>-58.000000</td>\n",
       "      <td>-59.000000</td>\n",
       "      <td>-61.000000</td>\n",
       "      <td>-58.000000</td>\n",
       "      <td>-58.000000</td>\n",
       "      <td>-58.000000</td>\n",
       "      <td>-57.000000</td>\n",
       "      <td>-59.000000</td>\n",
       "      <td>-59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-41.000000</td>\n",
       "      <td>-37.000000</td>\n",
       "      <td>-41.000000</td>\n",
       "      <td>-39.000000</td>\n",
       "      <td>-36.000000</td>\n",
       "      <td>-39.000000</td>\n",
       "      <td>-45.000000</td>\n",
       "      <td>-39.000000</td>\n",
       "      <td>-39.000000</td>\n",
       "      <td>-37.000000</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>-43.000000</td>\n",
       "      <td>-43.000000</td>\n",
       "      <td>-39.000000</td>\n",
       "      <td>-38.000000</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>-43.000000</td>\n",
       "      <td>-39.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 T1            T2            T3            T4            T5  \\\n",
       "count  36752.000000  36752.000000  36752.000000  36752.000000  36752.000000   \n",
       "mean     -65.865449    -64.521931    -64.574472    -65.296474    -64.462152   \n",
       "std        8.737286      8.914559      8.154517      8.142803      8.068375   \n",
       "min      -95.000000    -95.000000    -95.000000    -95.000000    -95.000000   \n",
       "25%      -72.000000    -70.000000    -70.000000    -70.000000    -69.000000   \n",
       "50%      -66.000000    -65.000000    -64.000000    -66.000000    -65.000000   \n",
       "75%      -61.000000    -59.000000    -59.000000    -61.000000    -59.000000   \n",
       "max      -41.000000    -37.000000    -41.000000    -39.000000    -36.000000   \n",
       "\n",
       "                 T6            T7            T8            T9           T10  \\\n",
       "count  36752.000000  36752.000000  36752.000000  36752.000000  36752.000000   \n",
       "mean     -63.318731    -67.030202    -66.591260    -65.692479    -65.555181   \n",
       "std        8.651501      9.257529     10.160193     10.598247     10.408147   \n",
       "min      -95.000000    -95.000000    -95.000000    -95.000000    -95.000000   \n",
       "25%      -69.000000    -74.000000    -75.000000    -73.000000    -73.000000   \n",
       "50%      -63.000000    -66.000000    -66.000000    -65.000000    -65.000000   \n",
       "75%      -57.000000    -60.000000    -59.000000    -58.000000    -58.000000   \n",
       "max      -39.000000    -45.000000    -39.000000    -39.000000    -37.000000   \n",
       "\n",
       "                T11           T12           T13           T14           T15  \\\n",
       "count  36752.000000  36752.000000  36752.000000  36752.000000  36752.000000   \n",
       "mean     -65.681922    -66.244204    -63.962614    -64.399080    -64.550011   \n",
       "std        9.453958      8.866239      8.254217      9.562839      9.207343   \n",
       "min      -95.000000    -95.000000    -95.000000    -95.000000    -95.000000   \n",
       "25%      -71.000000    -72.000000    -69.000000    -70.000000    -70.000000   \n",
       "50%      -65.000000    -66.000000    -63.000000    -64.000000    -65.000000   \n",
       "75%      -59.000000    -61.000000    -58.000000    -58.000000    -58.000000   \n",
       "max      -40.000000    -43.000000    -43.000000    -39.000000    -38.000000   \n",
       "\n",
       "                T16           T17           T18  \n",
       "count  36752.000000  36752.000000  36752.000000  \n",
       "mean     -64.136782    -65.910726    -66.590417  \n",
       "std       10.540542     10.071973     10.600992  \n",
       "min      -95.000000    -95.000000    -95.000000  \n",
       "25%      -71.000000    -72.000000    -74.000000  \n",
       "50%      -63.000000    -65.000000    -66.000000  \n",
       "75%      -57.000000    -59.000000    -59.000000  \n",
       "max      -40.000000    -43.000000    -39.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Analysis\n",
    "\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quzqZb_zO03N",
    "outputId": "225cb45f-fdc7-4540-b195-a73349820e9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T1        0\n",
       "T2        0\n",
       "T3        0\n",
       "T4        0\n",
       "T5        0\n",
       "T6        0\n",
       "T7        0\n",
       "T8        0\n",
       "T9        0\n",
       "T10       0\n",
       "T11       0\n",
       "T12       0\n",
       "T13       0\n",
       "T14       0\n",
       "T15       0\n",
       "T16       0\n",
       "T17       0\n",
       "T18       0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Null Values\n",
    "\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEUH5Z94O0zV",
    "outputId": "539e5167-ed73-4498-b71f-8497c0986747"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1267"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Duplicates\n",
    "\n",
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "B2jMODGfO0vq",
    "outputId": "5de06c49-d6a4-4571-e434-a716378d8304"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "      <th>T11</th>\n",
       "      <th>T12</th>\n",
       "      <th>T13</th>\n",
       "      <th>T14</th>\n",
       "      <th>T15</th>\n",
       "      <th>T16</th>\n",
       "      <th>T17</th>\n",
       "      <th>T18</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>-61</td>\n",
       "      <td>-60</td>\n",
       "      <td>-56</td>\n",
       "      <td>-55</td>\n",
       "      <td>-49</td>\n",
       "      <td>-62</td>\n",
       "      <td>-72</td>\n",
       "      <td>-63</td>\n",
       "      <td>-66</td>\n",
       "      <td>-68</td>\n",
       "      <td>-64</td>\n",
       "      <td>-61</td>\n",
       "      <td>-57</td>\n",
       "      <td>-59</td>\n",
       "      <td>-64</td>\n",
       "      <td>-65</td>\n",
       "      <td>-66</td>\n",
       "      <td>-68</td>\n",
       "      <td>B29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>-64</td>\n",
       "      <td>-70</td>\n",
       "      <td>-63</td>\n",
       "      <td>-68</td>\n",
       "      <td>-61</td>\n",
       "      <td>-57</td>\n",
       "      <td>-61</td>\n",
       "      <td>-56</td>\n",
       "      <td>-66</td>\n",
       "      <td>-70</td>\n",
       "      <td>-68</td>\n",
       "      <td>-65</td>\n",
       "      <td>-73</td>\n",
       "      <td>-65</td>\n",
       "      <td>-49</td>\n",
       "      <td>-45</td>\n",
       "      <td>-52</td>\n",
       "      <td>-50</td>\n",
       "      <td>A72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-67</td>\n",
       "      <td>-66</td>\n",
       "      <td>-63</td>\n",
       "      <td>-59</td>\n",
       "      <td>-61</td>\n",
       "      <td>-43</td>\n",
       "      <td>-64</td>\n",
       "      <td>-65</td>\n",
       "      <td>-82</td>\n",
       "      <td>-61</td>\n",
       "      <td>-59</td>\n",
       "      <td>-75</td>\n",
       "      <td>-57</td>\n",
       "      <td>-52</td>\n",
       "      <td>-53</td>\n",
       "      <td>-55</td>\n",
       "      <td>-59</td>\n",
       "      <td>-65</td>\n",
       "      <td>A48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>-67</td>\n",
       "      <td>-65</td>\n",
       "      <td>-63</td>\n",
       "      <td>-60</td>\n",
       "      <td>-62</td>\n",
       "      <td>-48</td>\n",
       "      <td>-64</td>\n",
       "      <td>-65</td>\n",
       "      <td>-80</td>\n",
       "      <td>-61</td>\n",
       "      <td>-58</td>\n",
       "      <td>-75</td>\n",
       "      <td>-57</td>\n",
       "      <td>-52</td>\n",
       "      <td>-53</td>\n",
       "      <td>-55</td>\n",
       "      <td>-59</td>\n",
       "      <td>-64</td>\n",
       "      <td>A48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>-65</td>\n",
       "      <td>-67</td>\n",
       "      <td>-65</td>\n",
       "      <td>-65</td>\n",
       "      <td>-60</td>\n",
       "      <td>-56</td>\n",
       "      <td>-78</td>\n",
       "      <td>-76</td>\n",
       "      <td>-78</td>\n",
       "      <td>-64</td>\n",
       "      <td>-62</td>\n",
       "      <td>-70</td>\n",
       "      <td>-57</td>\n",
       "      <td>-70</td>\n",
       "      <td>-59</td>\n",
       "      <td>-64</td>\n",
       "      <td>-79</td>\n",
       "      <td>-69</td>\n",
       "      <td>A34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36673</th>\n",
       "      <td>-64</td>\n",
       "      <td>-57</td>\n",
       "      <td>-70</td>\n",
       "      <td>-64</td>\n",
       "      <td>-77</td>\n",
       "      <td>-69</td>\n",
       "      <td>-79</td>\n",
       "      <td>-80</td>\n",
       "      <td>-78</td>\n",
       "      <td>-55</td>\n",
       "      <td>-47</td>\n",
       "      <td>-62</td>\n",
       "      <td>-62</td>\n",
       "      <td>-80</td>\n",
       "      <td>-69</td>\n",
       "      <td>-67</td>\n",
       "      <td>-79</td>\n",
       "      <td>-79</td>\n",
       "      <td>A18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36704</th>\n",
       "      <td>-63</td>\n",
       "      <td>-71</td>\n",
       "      <td>-65</td>\n",
       "      <td>-71</td>\n",
       "      <td>-77</td>\n",
       "      <td>-68</td>\n",
       "      <td>-95</td>\n",
       "      <td>-80</td>\n",
       "      <td>-75</td>\n",
       "      <td>-63</td>\n",
       "      <td>-68</td>\n",
       "      <td>-70</td>\n",
       "      <td>-81</td>\n",
       "      <td>-95</td>\n",
       "      <td>-75</td>\n",
       "      <td>-95</td>\n",
       "      <td>-82</td>\n",
       "      <td>-95</td>\n",
       "      <td>B5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36707</th>\n",
       "      <td>-64</td>\n",
       "      <td>-72</td>\n",
       "      <td>-63</td>\n",
       "      <td>-68</td>\n",
       "      <td>-61</td>\n",
       "      <td>-57</td>\n",
       "      <td>-62</td>\n",
       "      <td>-54</td>\n",
       "      <td>-65</td>\n",
       "      <td>-70</td>\n",
       "      <td>-67</td>\n",
       "      <td>-65</td>\n",
       "      <td>-72</td>\n",
       "      <td>-65</td>\n",
       "      <td>-49</td>\n",
       "      <td>-44</td>\n",
       "      <td>-50</td>\n",
       "      <td>-50</td>\n",
       "      <td>A72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36714</th>\n",
       "      <td>-64</td>\n",
       "      <td>-67</td>\n",
       "      <td>-63</td>\n",
       "      <td>-68</td>\n",
       "      <td>-69</td>\n",
       "      <td>-64</td>\n",
       "      <td>-60</td>\n",
       "      <td>-79</td>\n",
       "      <td>-67</td>\n",
       "      <td>-75</td>\n",
       "      <td>-71</td>\n",
       "      <td>-63</td>\n",
       "      <td>-58</td>\n",
       "      <td>-51</td>\n",
       "      <td>-50</td>\n",
       "      <td>-55</td>\n",
       "      <td>-61</td>\n",
       "      <td>-57</td>\n",
       "      <td>A49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36746</th>\n",
       "      <td>-60</td>\n",
       "      <td>-61</td>\n",
       "      <td>-53</td>\n",
       "      <td>-54</td>\n",
       "      <td>-68</td>\n",
       "      <td>-60</td>\n",
       "      <td>-78</td>\n",
       "      <td>-69</td>\n",
       "      <td>-73</td>\n",
       "      <td>-69</td>\n",
       "      <td>-66</td>\n",
       "      <td>-56</td>\n",
       "      <td>-49</td>\n",
       "      <td>-67</td>\n",
       "      <td>-62</td>\n",
       "      <td>-72</td>\n",
       "      <td>-79</td>\n",
       "      <td>-68</td>\n",
       "      <td>A30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1267 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       T1  T2  T3  T4  T5  T6  T7  T8  T9  T10  T11  T12  T13  T14  T15  T16  \\\n",
       "870   -61 -60 -56 -55 -49 -62 -72 -63 -66  -68  -64  -61  -57  -59  -64  -65   \n",
       "889   -64 -70 -63 -68 -61 -57 -61 -56 -66  -70  -68  -65  -73  -65  -49  -45   \n",
       "995   -67 -66 -63 -59 -61 -43 -64 -65 -82  -61  -59  -75  -57  -52  -53  -55   \n",
       "1084  -67 -65 -63 -60 -62 -48 -64 -65 -80  -61  -58  -75  -57  -52  -53  -55   \n",
       "1887  -65 -67 -65 -65 -60 -56 -78 -76 -78  -64  -62  -70  -57  -70  -59  -64   \n",
       "...    ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...  ...  ...  ...  ...   \n",
       "36673 -64 -57 -70 -64 -77 -69 -79 -80 -78  -55  -47  -62  -62  -80  -69  -67   \n",
       "36704 -63 -71 -65 -71 -77 -68 -95 -80 -75  -63  -68  -70  -81  -95  -75  -95   \n",
       "36707 -64 -72 -63 -68 -61 -57 -62 -54 -65  -70  -67  -65  -72  -65  -49  -44   \n",
       "36714 -64 -67 -63 -68 -69 -64 -60 -79 -67  -75  -71  -63  -58  -51  -50  -55   \n",
       "36746 -60 -61 -53 -54 -68 -60 -78 -69 -73  -69  -66  -56  -49  -67  -62  -72   \n",
       "\n",
       "       T17  T18 target  \n",
       "870    -66  -68    B29  \n",
       "889    -52  -50    A72  \n",
       "995    -59  -65    A48  \n",
       "1084   -59  -64    A48  \n",
       "1887   -79  -69    A34  \n",
       "...    ...  ...    ...  \n",
       "36673  -79  -79    A18  \n",
       "36704  -82  -95     B5  \n",
       "36707  -50  -50    A72  \n",
       "36714  -61  -57    A49  \n",
       "36746  -79  -68    A30  \n",
       "\n",
       "[1267 rows x 19 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZfomzBLO99u"
   },
   "source": [
    "Duplicates can affect the results of clustering algorithms because they artificially inflate the importance of certain data points. If the duplicates are not removed, the clustering algorithm may assign undue weight to those points, potentially biasing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pXrSr5BtO91g"
   },
   "outputs": [],
   "source": [
    "train_df=train_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EiAwxwr-PLIf",
    "outputId": "83f130e7-5140-4b7e-c3b9-7778b399e12d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "d-Bemo-nPLFP"
   },
   "outputs": [],
   "source": [
    "x_train= train_df.iloc[:, :-1]\n",
    "y_train= train_df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUHT7B1tYGCQ",
    "outputId": "955375c2-e25c-4f8f-c4d8-e112dfcf4cc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B37', 'B61', 'A19', 'A22', 'A33', 'A75', 'A41', 'B14', 'B80',\n",
       "       'B64', 'A1', 'B21', 'A70', 'B57', 'B9', 'B50', 'B31', 'A21', 'B30',\n",
       "       'A4', 'B40', 'B6', 'A64', 'A25', 'B18', 'A66', 'A53', 'B73', 'A50',\n",
       "       'B25', 'A29', 'A35', 'A3', 'A46', 'B45', 'A36', 'B5', 'A79', 'A32',\n",
       "       'B23', 'A7', 'A62', 'A71', 'A58', 'A9', 'B63', 'B51', 'B8', 'B36',\n",
       "       'A14', 'A15', 'A24', 'B38', 'A10', 'B4', 'A37', 'A12', 'B41',\n",
       "       'A17', 'B49', 'B71', 'B59', 'B39', 'B29', 'A76', 'B58', 'B28',\n",
       "       'A77', 'A67', 'B24', 'A31', 'A34', 'A5', 'B34', 'A11', 'B16',\n",
       "       'B20', 'A63', 'B53', 'A73', 'A44', 'A69', 'A56', 'A54', 'A55',\n",
       "       'B11', 'A42', 'B22', 'B67', 'A74', 'A57', 'A8', 'B46', 'B15',\n",
       "       'A51', 'A40', 'A59', 'A72', 'B32', 'A2', 'A13', 'B17', 'A65',\n",
       "       'A52', 'A47', 'B47', 'B77', 'B3', 'B35', 'B44', 'A80', 'B43',\n",
       "       'B54', 'B75', 'A49', 'B52', 'B33', 'B2', 'B1', 'A39', 'B7', 'A20',\n",
       "       'A18', 'B12', 'A27', 'A6', 'A68', 'B72', 'B66', 'B69', 'A26',\n",
       "       'A78', 'A60', 'A16', 'B19', 'B62', 'B27', 'B68', 'B76', 'B65',\n",
       "       'B55', 'A45', 'A30', 'A23', 'B42', 'A38', 'B10', 'A48', 'B70',\n",
       "       'A43', 'B74', 'B13', 'B56', 'B48', 'B60', 'B79', 'B78', 'A28',\n",
       "       'B26', 'A61'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5d5aZY5_HHi"
   },
   "source": [
    "###### Most classifiers in scikit-learn expect the target variable to be numeric, not categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1J66U6TKlu_"
   },
   "source": [
    "##### Label encoding is typically used for encoding categorical target variables (y_train) when performing classification tasks, especially when there are more than two classes. One-hot encoding is not usually applied to target variables because it can introduce multicollinearity issues and may not be suitable for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QVEUcL6M-ZO6"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RBSmCQhC-i2j"
   },
   "outputs": [],
   "source": [
    "y_train_encoded = label_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "014zieV3gBeP"
   },
   "source": [
    "# Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gs2auNjjcyeT"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arSYz3lLgKYl"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bp6aJ0GyFKAe"
   },
   "outputs": [],
   "source": [
    "numeric_features = x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8A-MCqjCEpsP"
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features)],\n",
    "         remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aYy-06PvoESl"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Random Forest': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ]),\n",
    "    'SVC': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', SVC())\n",
    "    ]),\n",
    "    'XGBoost Classifier': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', XGBClassifier())\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUPDHvmYoEPN",
    "outputId": "87c1fd47-43fd-4f30-e8cc-cf53ce142816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       157\n",
      "           1       1.00      1.00      1.00       171\n",
      "           2       1.00      1.00      1.00       173\n",
      "           3       1.00      1.00      1.00       167\n",
      "           4       1.00      1.00      1.00       167\n",
      "           5       1.00      1.00      1.00       321\n",
      "           6       1.00      1.00      1.00       325\n",
      "           7       1.00      1.00      1.00       160\n",
      "           8       1.00      1.00      1.00       160\n",
      "           9       1.00      1.00      1.00        86\n",
      "          10       1.00      1.00      1.00       159\n",
      "          11       1.00      1.00      1.00       104\n",
      "          12       1.00      1.00      1.00       173\n",
      "          13       1.00      1.00      1.00       298\n",
      "          14       1.00      1.00      1.00       177\n",
      "          15       1.00      1.00      1.00       163\n",
      "          16       1.00      1.00      1.00       173\n",
      "          17       1.00      1.00      1.00       333\n",
      "          18       1.00      1.00      1.00       169\n",
      "          19       1.00      1.00      1.00       152\n",
      "          20       1.00      1.00      1.00       176\n",
      "          21       1.00      1.00      1.00       341\n",
      "          22       1.00      1.00      1.00       340\n",
      "          23       1.00      1.00      1.00       166\n",
      "          24       0.99      0.98      0.98       163\n",
      "          25       1.00      1.00      1.00       151\n",
      "          26       1.00      1.00      1.00       456\n",
      "          27       1.00      1.00      1.00       269\n",
      "          28       1.00      1.00      1.00       162\n",
      "          29       1.00      1.00      1.00       158\n",
      "          30       1.00      1.00      1.00       321\n",
      "          31       1.00      1.00      1.00       157\n",
      "          32       0.99      1.00      1.00       505\n",
      "          33       1.00      1.00      1.00       326\n",
      "          34       1.00      1.00      1.00       119\n",
      "          35       1.00      1.00      1.00       164\n",
      "          36       1.00      1.00      1.00       164\n",
      "          37       1.00      1.00      1.00       170\n",
      "          38       1.00      1.00      1.00       177\n",
      "          39       1.00      1.00      1.00       160\n",
      "          40       1.00      1.00      1.00       147\n",
      "          41       1.00      1.00      1.00       178\n",
      "          42       1.00      1.00      1.00        77\n",
      "          43       1.00      1.00      1.00       145\n",
      "          44       1.00      1.00      1.00       177\n",
      "          45       1.00      1.00      1.00       168\n",
      "          46       1.00      1.00      1.00       177\n",
      "          47       1.00      1.00      1.00       165\n",
      "          48       1.00      1.00      1.00       175\n",
      "          49       1.00      1.00      1.00       172\n",
      "          50       1.00      1.00      1.00       159\n",
      "          51       1.00      1.00      1.00       168\n",
      "          52       1.00      1.00      1.00       162\n",
      "          53       1.00      1.00      1.00       150\n",
      "          54       1.00      1.00      1.00       177\n",
      "          55       1.00      1.00      1.00       161\n",
      "          56       1.00      1.00      1.00       156\n",
      "          57       0.99      0.96      0.98       168\n",
      "          58       1.00      1.00      1.00       175\n",
      "          59       1.00      1.00      1.00       172\n",
      "          60       1.00      1.00      1.00       172\n",
      "          61       1.00      0.99      1.00       165\n",
      "          62       1.00      1.00      1.00       164\n",
      "          63       1.00      1.00      1.00       176\n",
      "          64       1.00      1.00      1.00       171\n",
      "          65       1.00      1.00      1.00       139\n",
      "          66       1.00      1.00      1.00       173\n",
      "          67       1.00      1.00      1.00       168\n",
      "          68       1.00      1.00      1.00       158\n",
      "          69       0.99      1.00      0.99        87\n",
      "          70       1.00      1.00      1.00       169\n",
      "          71       1.00      1.00      1.00       112\n",
      "          72       1.00      1.00      1.00       186\n",
      "          73       1.00      1.00      1.00       184\n",
      "          74       1.00      1.00      1.00       176\n",
      "          75       1.00      1.00      1.00       174\n",
      "          76       1.00      1.00      1.00       165\n",
      "          77       1.00      1.00      1.00       175\n",
      "          78       1.00      1.00      1.00       152\n",
      "          79       1.00      1.00      1.00       138\n",
      "          80       1.00      1.00      1.00       163\n",
      "          81       1.00      1.00      1.00       156\n",
      "          82       1.00      1.00      1.00       171\n",
      "          83       1.00      1.00      1.00       164\n",
      "          84       0.99      1.00      1.00       145\n",
      "          85       1.00      1.00      1.00       167\n",
      "          86       1.00      1.00      1.00       158\n",
      "          87       1.00      1.00      1.00       177\n",
      "          88       0.97      0.99      0.98       176\n",
      "          89       1.00      1.00      1.00       166\n",
      "          90       1.00      1.00      1.00       163\n",
      "          91       1.00      1.00      1.00       181\n",
      "          92       1.00      1.00      1.00       167\n",
      "          93       1.00      1.00      1.00       172\n",
      "          94       1.00      1.00      1.00       174\n",
      "          95       1.00      1.00      1.00        66\n",
      "          96       1.00      1.00      1.00       167\n",
      "          97       1.00      1.00      1.00       167\n",
      "          98       1.00      1.00      1.00       168\n",
      "          99       1.00      1.00      1.00       170\n",
      "         100       1.00      1.00      1.00       163\n",
      "         101       1.00      1.00      1.00       160\n",
      "         102       1.00      1.00      1.00       177\n",
      "         103       1.00      1.00      1.00       160\n",
      "         104       1.00      1.00      1.00       175\n",
      "         105       1.00      1.00      1.00       157\n",
      "         106       1.00      1.00      1.00       172\n",
      "         107       1.00      1.00      1.00       173\n",
      "         108       1.00      1.00      1.00       177\n",
      "         109       1.00      1.00      1.00       156\n",
      "         110       1.00      1.00      1.00       171\n",
      "         111       1.00      1.00      1.00       160\n",
      "         112       1.00      1.00      1.00       165\n",
      "         113       1.00      1.00      1.00       178\n",
      "         114       1.00      1.00      1.00       168\n",
      "         115       1.00      1.00      1.00       160\n",
      "         116       1.00      1.00      1.00       175\n",
      "         117       1.00      1.00      1.00       164\n",
      "         118       1.00      1.00      1.00       178\n",
      "         119       1.00      1.00      1.00       336\n",
      "         120       1.00      1.00      1.00       161\n",
      "         121       1.00      1.00      1.00       167\n",
      "         122       1.00      1.00      1.00       189\n",
      "         123       1.00      1.00      1.00       169\n",
      "         124       1.00      1.00      1.00        88\n",
      "         125       1.00      1.00      1.00       166\n",
      "         126       1.00      1.00      1.00       185\n",
      "         127       1.00      1.00      1.00       167\n",
      "         128       1.00      1.00      1.00       176\n",
      "         129       1.00      1.00      1.00       168\n",
      "         130       1.00      1.00      1.00       163\n",
      "         131       1.00      1.00      1.00       162\n",
      "         132       1.00      1.00      1.00       171\n",
      "         133       1.00      1.00      1.00       310\n",
      "         134       1.00      1.00      1.00       157\n",
      "         135       1.00      1.00      1.00       171\n",
      "         136       1.00      1.00      1.00       162\n",
      "         137       1.00      1.00      1.00       157\n",
      "         138       1.00      1.00      1.00       178\n",
      "         139       1.00      1.00      1.00       174\n",
      "         140       1.00      1.00      1.00       162\n",
      "         141       1.00      1.00      1.00       153\n",
      "         142       1.00      1.00      1.00       149\n",
      "         143       1.00      1.00      1.00       175\n",
      "         144       1.00      1.00      1.00       159\n",
      "         145       1.00      1.00      1.00       166\n",
      "         146       1.00      1.00      1.00       189\n",
      "         147       1.00      1.00      1.00       166\n",
      "         148       1.00      1.00      1.00       171\n",
      "         149       1.00      1.00      1.00       169\n",
      "         150       1.00      1.00      1.00       161\n",
      "         151       1.00      1.00      1.00       157\n",
      "         152       1.00      1.00      1.00       164\n",
      "         153       1.00      1.00      1.00       183\n",
      "         154       1.00      1.00      1.00       158\n",
      "         155       1.00      1.00      1.00       159\n",
      "         156       1.00      1.00      1.00       179\n",
      "         157       1.00      1.00      1.00       175\n",
      "         158       1.00      1.00      1.00       157\n",
      "         159       1.00      1.00      1.00       166\n",
      "\n",
      "    accuracy                           1.00     28388\n",
      "   macro avg       1.00      1.00      1.00     28388\n",
      "weighted avg       1.00      1.00      1.00     28388\n",
      "\n",
      "Training SVC...\n",
      "Classification Report for SVC:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94       157\n",
      "           1       0.81      0.99      0.89       171\n",
      "           2       0.96      1.00      0.98       173\n",
      "           3       1.00      1.00      1.00       167\n",
      "           4       1.00      1.00      1.00       167\n",
      "           5       1.00      1.00      1.00       321\n",
      "           6       0.99      0.96      0.98       325\n",
      "           7       1.00      1.00      1.00       160\n",
      "           8       0.99      0.99      0.99       160\n",
      "           9       1.00      1.00      1.00        86\n",
      "          10       0.99      0.99      0.99       159\n",
      "          11       1.00      1.00      1.00       104\n",
      "          12       0.99      1.00      1.00       173\n",
      "          13       0.95      0.99      0.97       298\n",
      "          14       1.00      1.00      1.00       177\n",
      "          15       1.00      1.00      1.00       163\n",
      "          16       1.00      1.00      1.00       173\n",
      "          17       0.97      0.97      0.97       333\n",
      "          18       1.00      1.00      1.00       169\n",
      "          19       1.00      1.00      1.00       152\n",
      "          20       1.00      1.00      1.00       176\n",
      "          21       0.99      0.98      0.99       341\n",
      "          22       0.99      0.88      0.93       340\n",
      "          23       1.00      1.00      1.00       166\n",
      "          24       0.83      1.00      0.91       163\n",
      "          25       1.00      1.00      1.00       151\n",
      "          26       0.98      0.98      0.98       456\n",
      "          27       1.00      1.00      1.00       269\n",
      "          28       1.00      1.00      1.00       162\n",
      "          29       1.00      1.00      1.00       158\n",
      "          30       0.99      0.97      0.98       321\n",
      "          31       1.00      1.00      1.00       157\n",
      "          32       0.99      0.92      0.95       505\n",
      "          33       0.99      1.00      1.00       326\n",
      "          34       1.00      1.00      1.00       119\n",
      "          35       0.98      1.00      0.99       164\n",
      "          36       1.00      1.00      1.00       164\n",
      "          37       1.00      1.00      1.00       170\n",
      "          38       1.00      1.00      1.00       177\n",
      "          39       0.99      0.98      0.99       160\n",
      "          40       1.00      1.00      1.00       147\n",
      "          41       1.00      1.00      1.00       178\n",
      "          42       1.00      1.00      1.00        77\n",
      "          43       0.98      0.98      0.98       145\n",
      "          44       0.94      0.96      0.95       177\n",
      "          45       1.00      1.00      1.00       168\n",
      "          46       1.00      1.00      1.00       177\n",
      "          47       1.00      1.00      1.00       165\n",
      "          48       0.99      0.99      0.99       175\n",
      "          49       1.00      1.00      1.00       172\n",
      "          50       1.00      1.00      1.00       159\n",
      "          51       1.00      1.00      1.00       168\n",
      "          52       1.00      1.00      1.00       162\n",
      "          53       1.00      1.00      1.00       150\n",
      "          54       1.00      1.00      1.00       177\n",
      "          55       1.00      1.00      1.00       161\n",
      "          56       1.00      1.00      1.00       156\n",
      "          57       0.96      0.51      0.66       168\n",
      "          58       1.00      1.00      1.00       175\n",
      "          59       1.00      1.00      1.00       172\n",
      "          60       1.00      1.00      1.00       172\n",
      "          61       0.99      0.98      0.98       165\n",
      "          62       1.00      1.00      1.00       164\n",
      "          63       1.00      1.00      1.00       176\n",
      "          64       1.00      1.00      1.00       171\n",
      "          65       1.00      1.00      1.00       139\n",
      "          66       0.99      0.99      0.99       173\n",
      "          67       1.00      1.00      1.00       168\n",
      "          68       1.00      1.00      1.00       158\n",
      "          69       0.99      1.00      0.99        87\n",
      "          70       1.00      0.99      1.00       169\n",
      "          71       1.00      0.99      1.00       112\n",
      "          72       1.00      1.00      1.00       186\n",
      "          73       1.00      1.00      1.00       184\n",
      "          74       0.98      0.99      0.99       176\n",
      "          75       1.00      1.00      1.00       174\n",
      "          76       1.00      1.00      1.00       165\n",
      "          77       1.00      1.00      1.00       175\n",
      "          78       1.00      1.00      1.00       152\n",
      "          79       0.98      0.94      0.96       138\n",
      "          80       1.00      1.00      1.00       163\n",
      "          81       1.00      1.00      1.00       156\n",
      "          82       1.00      1.00      1.00       171\n",
      "          83       1.00      1.00      1.00       164\n",
      "          84       0.95      1.00      0.98       145\n",
      "          85       1.00      1.00      1.00       167\n",
      "          86       1.00      1.00      1.00       158\n",
      "          87       1.00      1.00      1.00       177\n",
      "          88       0.66      0.93      0.77       176\n",
      "          89       1.00      1.00      1.00       166\n",
      "          90       1.00      1.00      1.00       163\n",
      "          91       1.00      1.00      1.00       181\n",
      "          92       1.00      0.99      1.00       167\n",
      "          93       1.00      1.00      1.00       172\n",
      "          94       1.00      1.00      1.00       174\n",
      "          95       1.00      1.00      1.00        66\n",
      "          96       1.00      1.00      1.00       167\n",
      "          97       1.00      1.00      1.00       167\n",
      "          98       1.00      1.00      1.00       168\n",
      "          99       1.00      1.00      1.00       170\n",
      "         100       1.00      1.00      1.00       163\n",
      "         101       0.99      1.00      1.00       160\n",
      "         102       1.00      1.00      1.00       177\n",
      "         103       1.00      1.00      1.00       160\n",
      "         104       1.00      1.00      1.00       175\n",
      "         105       1.00      1.00      1.00       157\n",
      "         106       1.00      1.00      1.00       172\n",
      "         107       1.00      1.00      1.00       173\n",
      "         108       1.00      1.00      1.00       177\n",
      "         109       1.00      1.00      1.00       156\n",
      "         110       1.00      1.00      1.00       171\n",
      "         111       1.00      1.00      1.00       160\n",
      "         112       1.00      1.00      1.00       165\n",
      "         113       0.99      1.00      1.00       178\n",
      "         114       1.00      1.00      1.00       168\n",
      "         115       1.00      1.00      1.00       160\n",
      "         116       0.99      1.00      1.00       175\n",
      "         117       1.00      1.00      1.00       164\n",
      "         118       1.00      1.00      1.00       178\n",
      "         119       1.00      1.00      1.00       336\n",
      "         120       1.00      1.00      1.00       161\n",
      "         121       1.00      1.00      1.00       167\n",
      "         122       1.00      1.00      1.00       189\n",
      "         123       1.00      1.00      1.00       169\n",
      "         124       1.00      1.00      1.00        88\n",
      "         125       1.00      1.00      1.00       166\n",
      "         126       1.00      1.00      1.00       185\n",
      "         127       1.00      1.00      1.00       167\n",
      "         128       0.99      0.98      0.99       176\n",
      "         129       1.00      1.00      1.00       168\n",
      "         130       1.00      1.00      1.00       163\n",
      "         131       1.00      1.00      1.00       162\n",
      "         132       0.98      1.00      0.99       171\n",
      "         133       1.00      1.00      1.00       310\n",
      "         134       1.00      1.00      1.00       157\n",
      "         135       1.00      1.00      1.00       171\n",
      "         136       1.00      1.00      1.00       162\n",
      "         137       1.00      0.99      1.00       157\n",
      "         138       1.00      1.00      1.00       178\n",
      "         139       1.00      1.00      1.00       174\n",
      "         140       1.00      1.00      1.00       162\n",
      "         141       1.00      0.93      0.97       153\n",
      "         142       1.00      1.00      1.00       149\n",
      "         143       1.00      1.00      1.00       175\n",
      "         144       1.00      1.00      1.00       159\n",
      "         145       0.94      0.99      0.97       166\n",
      "         146       1.00      1.00      1.00       189\n",
      "         147       1.00      1.00      1.00       166\n",
      "         148       1.00      1.00      1.00       171\n",
      "         149       1.00      1.00      1.00       169\n",
      "         150       1.00      0.99      1.00       161\n",
      "         151       1.00      1.00      1.00       157\n",
      "         152       1.00      1.00      1.00       164\n",
      "         153       1.00      1.00      1.00       183\n",
      "         154       0.99      1.00      1.00       158\n",
      "         155       1.00      1.00      1.00       159\n",
      "         156       1.00      1.00      1.00       179\n",
      "         157       1.00      1.00      1.00       175\n",
      "         158       1.00      1.00      1.00       157\n",
      "         159       1.00      1.00      1.00       166\n",
      "\n",
      "    accuracy                           0.99     28388\n",
      "   macro avg       0.99      0.99      0.99     28388\n",
      "weighted avg       0.99      0.99      0.99     28388\n",
      "\n",
      "Training XGBoost Classifier...\n",
      "Classification Report for XGBoost Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       157\n",
      "           1       1.00      1.00      1.00       171\n",
      "           2       1.00      1.00      1.00       173\n",
      "           3       1.00      1.00      1.00       167\n",
      "           4       1.00      1.00      1.00       167\n",
      "           5       1.00      1.00      1.00       321\n",
      "           6       1.00      1.00      1.00       325\n",
      "           7       1.00      1.00      1.00       160\n",
      "           8       1.00      1.00      1.00       160\n",
      "           9       1.00      1.00      1.00        86\n",
      "          10       1.00      1.00      1.00       159\n",
      "          11       1.00      1.00      1.00       104\n",
      "          12       1.00      1.00      1.00       173\n",
      "          13       1.00      1.00      1.00       298\n",
      "          14       1.00      1.00      1.00       177\n",
      "          15       1.00      1.00      1.00       163\n",
      "          16       1.00      1.00      1.00       173\n",
      "          17       1.00      1.00      1.00       333\n",
      "          18       1.00      1.00      1.00       169\n",
      "          19       1.00      0.99      1.00       152\n",
      "          20       1.00      1.00      1.00       176\n",
      "          21       1.00      1.00      1.00       341\n",
      "          22       1.00      1.00      1.00       340\n",
      "          23       1.00      1.00      1.00       166\n",
      "          24       0.97      1.00      0.98       163\n",
      "          25       1.00      1.00      1.00       151\n",
      "          26       1.00      1.00      1.00       456\n",
      "          27       1.00      1.00      1.00       269\n",
      "          28       1.00      1.00      1.00       162\n",
      "          29       1.00      1.00      1.00       158\n",
      "          30       1.00      1.00      1.00       321\n",
      "          31       1.00      1.00      1.00       157\n",
      "          32       1.00      0.99      1.00       505\n",
      "          33       1.00      1.00      1.00       326\n",
      "          34       1.00      1.00      1.00       119\n",
      "          35       1.00      1.00      1.00       164\n",
      "          36       1.00      1.00      1.00       164\n",
      "          37       1.00      1.00      1.00       170\n",
      "          38       1.00      1.00      1.00       177\n",
      "          39       1.00      1.00      1.00       160\n",
      "          40       1.00      1.00      1.00       147\n",
      "          41       1.00      1.00      1.00       178\n",
      "          42       1.00      1.00      1.00        77\n",
      "          43       1.00      1.00      1.00       145\n",
      "          44       1.00      1.00      1.00       177\n",
      "          45       1.00      1.00      1.00       168\n",
      "          46       1.00      1.00      1.00       177\n",
      "          47       1.00      1.00      1.00       165\n",
      "          48       1.00      1.00      1.00       175\n",
      "          49       1.00      1.00      1.00       172\n",
      "          50       1.00      1.00      1.00       159\n",
      "          51       1.00      1.00      1.00       168\n",
      "          52       1.00      1.00      1.00       162\n",
      "          53       1.00      1.00      1.00       150\n",
      "          54       1.00      0.99      1.00       177\n",
      "          55       1.00      1.00      1.00       161\n",
      "          56       1.00      1.00      1.00       156\n",
      "          57       0.99      0.96      0.98       168\n",
      "          58       1.00      1.00      1.00       175\n",
      "          59       0.99      1.00      1.00       172\n",
      "          60       1.00      1.00      1.00       172\n",
      "          61       1.00      0.99      1.00       165\n",
      "          62       1.00      1.00      1.00       164\n",
      "          63       1.00      1.00      1.00       176\n",
      "          64       1.00      1.00      1.00       171\n",
      "          65       1.00      1.00      1.00       139\n",
      "          66       1.00      1.00      1.00       173\n",
      "          67       1.00      1.00      1.00       168\n",
      "          68       1.00      1.00      1.00       158\n",
      "          69       1.00      0.99      0.99        87\n",
      "          70       1.00      1.00      1.00       169\n",
      "          71       1.00      1.00      1.00       112\n",
      "          72       1.00      1.00      1.00       186\n",
      "          73       1.00      1.00      1.00       184\n",
      "          74       1.00      1.00      1.00       176\n",
      "          75       1.00      1.00      1.00       174\n",
      "          76       1.00      1.00      1.00       165\n",
      "          77       1.00      1.00      1.00       175\n",
      "          78       1.00      1.00      1.00       152\n",
      "          79       1.00      1.00      1.00       138\n",
      "          80       1.00      1.00      1.00       163\n",
      "          81       1.00      1.00      1.00       156\n",
      "          82       1.00      1.00      1.00       171\n",
      "          83       1.00      1.00      1.00       164\n",
      "          84       0.99      1.00      1.00       145\n",
      "          85       1.00      1.00      1.00       167\n",
      "          86       1.00      1.00      1.00       158\n",
      "          87       1.00      1.00      1.00       177\n",
      "          88       0.96      0.99      0.98       176\n",
      "          89       1.00      1.00      1.00       166\n",
      "          90       1.00      1.00      1.00       163\n",
      "          91       1.00      1.00      1.00       181\n",
      "          92       1.00      1.00      1.00       167\n",
      "          93       1.00      1.00      1.00       172\n",
      "          94       1.00      1.00      1.00       174\n",
      "          95       1.00      1.00      1.00        66\n",
      "          96       1.00      1.00      1.00       167\n",
      "          97       1.00      1.00      1.00       167\n",
      "          98       1.00      1.00      1.00       168\n",
      "          99       1.00      1.00      1.00       170\n",
      "         100       1.00      1.00      1.00       163\n",
      "         101       1.00      1.00      1.00       160\n",
      "         102       1.00      1.00      1.00       177\n",
      "         103       1.00      1.00      1.00       160\n",
      "         104       1.00      1.00      1.00       175\n",
      "         105       1.00      1.00      1.00       157\n",
      "         106       1.00      1.00      1.00       172\n",
      "         107       1.00      1.00      1.00       173\n",
      "         108       1.00      1.00      1.00       177\n",
      "         109       1.00      1.00      1.00       156\n",
      "         110       1.00      1.00      1.00       171\n",
      "         111       1.00      1.00      1.00       160\n",
      "         112       1.00      1.00      1.00       165\n",
      "         113       1.00      1.00      1.00       178\n",
      "         114       1.00      1.00      1.00       168\n",
      "         115       1.00      1.00      1.00       160\n",
      "         116       1.00      1.00      1.00       175\n",
      "         117       1.00      1.00      1.00       164\n",
      "         118       1.00      1.00      1.00       178\n",
      "         119       1.00      1.00      1.00       336\n",
      "         120       1.00      1.00      1.00       161\n",
      "         121       1.00      1.00      1.00       167\n",
      "         122       1.00      1.00      1.00       189\n",
      "         123       1.00      1.00      1.00       169\n",
      "         124       1.00      1.00      1.00        88\n",
      "         125       1.00      1.00      1.00       166\n",
      "         126       1.00      1.00      1.00       185\n",
      "         127       1.00      1.00      1.00       167\n",
      "         128       1.00      1.00      1.00       176\n",
      "         129       1.00      1.00      1.00       168\n",
      "         130       1.00      1.00      1.00       163\n",
      "         131       1.00      1.00      1.00       162\n",
      "         132       1.00      1.00      1.00       171\n",
      "         133       1.00      1.00      1.00       310\n",
      "         134       1.00      1.00      1.00       157\n",
      "         135       1.00      1.00      1.00       171\n",
      "         136       1.00      1.00      1.00       162\n",
      "         137       1.00      1.00      1.00       157\n",
      "         138       1.00      1.00      1.00       178\n",
      "         139       1.00      1.00      1.00       174\n",
      "         140       1.00      1.00      1.00       162\n",
      "         141       1.00      1.00      1.00       153\n",
      "         142       1.00      1.00      1.00       149\n",
      "         143       1.00      1.00      1.00       175\n",
      "         144       1.00      1.00      1.00       159\n",
      "         145       1.00      1.00      1.00       166\n",
      "         146       1.00      1.00      1.00       189\n",
      "         147       1.00      1.00      1.00       166\n",
      "         148       1.00      1.00      1.00       171\n",
      "         149       1.00      1.00      1.00       169\n",
      "         150       1.00      1.00      1.00       161\n",
      "         151       1.00      1.00      1.00       157\n",
      "         152       1.00      1.00      1.00       164\n",
      "         153       1.00      1.00      1.00       183\n",
      "         154       1.00      1.00      1.00       158\n",
      "         155       1.00      1.00      1.00       159\n",
      "         156       1.00      1.00      1.00       179\n",
      "         157       1.00      1.00      1.00       175\n",
      "         158       1.00      1.00      1.00       157\n",
      "         159       1.00      1.00      1.00       166\n",
      "\n",
      "    accuracy                           1.00     28388\n",
      "   macro avg       1.00      1.00      1.00     28388\n",
      "weighted avg       1.00      1.00      1.00     28388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained_models = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.fit(x_train, y_train)\n",
    "    trained_models[model_name] = model\n",
    "    # Generate classification report\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    print(f\"Classification Report for {model_name}:\\n\", classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Tv5PTGvCoELv"
   },
   "outputs": [],
   "source": [
    "test_predictions = {}\n",
    "for model_name, model in trained_models.items():\n",
    "    predictions = model.predict(test_df)\n",
    "    test_predictions[model_name] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1G_Cecb4f0S",
    "outputId": "96ce4df8-16c6-4e59-84c4-2b0a5c24e3de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Random Forest:\n",
      "[151   1 141 ... 145  31  78]\n",
      "Predictions for SVC:\n",
      "[151   1 141 ... 145  31  78]\n",
      "Predictions for XGBoost Classifier:\n",
      "[151   1 141 ... 145  31  78]\n"
     ]
    }
   ],
   "source": [
    "for model_name, predictions in test_predictions.items():\n",
    "    print(f\"Predictions for {model_name}:\")\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2_output = {\n",
    "    'Random Forest Predictions': test_predictions['Random Forest'],\n",
    "    'SVC Predictions': test_predictions['SVC'],\n",
    "    'XGBoost Classifier Predictions': test_predictions['XGBoost Classifier']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task2 = pd.DataFrame(task2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task2.to_csv('task2_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
